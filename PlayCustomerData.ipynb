{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Customer Data\n",
    "  Should be applicable to all customer data sets,\n",
    "  Explores categories as well as time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import datasets, svm, metrics\n",
    "from pandas import DataFrame\n",
    "import matplotlib as mpl\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "# from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# get nltk and corpus\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# get scapy and corpus\n",
    "# import spacy\n",
    "# import time\n",
    "# from functools import lru_cache\n",
    "import seaborn as sns\n",
    "import humanize\n",
    "\n",
    "# import swifter\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from IPython.display import display\n",
    "\n",
    "from humanize import intcomma, intword\n",
    "# import pandas_profiling\n",
    "import arrow\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "from functools import partial\n",
    "from pandas_util import time_it\n",
    "import altair as alt\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the plot wider\n",
    "height_in_inches = 10\n",
    "matplotlib.rc(\"figure\", figsize=(2 * height_in_inches, height_in_inches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#raw_csv = \"~/data/wamd.all.csv\"\n",
    "# raw_csv = \"~/data/imessage.csv\"\n",
    "raw_csv = \"~/data/sergio.csv\"\n",
    "cleaned_df_pickled = f\"{os.path.expanduser(raw_csv)}.pickle.gz\"\n",
    "\n",
    "\n",
    "# Load+Clean+Explore data using Dask as it's got multi-core.\n",
    "# Then convert to a pandas dataframe pickle.\n",
    "# dask_df = dd.read_csv(raw_csv,sep=',' )\n",
    "#df = dask_df.compute()\n",
    "df = pd.read_csv(raw_csv,sep=',')\n",
    "# df = df.compute()\n",
    "# df = pd.read_csv(raw_csv,sep='\\t')\n",
    "# df = pd.read_csv(raw_csv, sep=\"|\", lineterminator=\"\\n\", error_bad_lines=False)\n",
    "# df = pd.read_csv(raw_csv, sep=\"\\t\", lineterminator='\\r\"')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.compute()\n",
    "#df.to_pickle(cleaned_df_pickled)\n",
    "#ti = time_it(f\"Load dataframe:{cleaned_df_pickled}\")\n",
    "#df = pd.read_pickle(cleaned_df_pickled)\n",
    "# ti.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isWorkChat = True\n",
    "isImessage = False\n",
    "# clean up some  data\n",
    "\n",
    "datetimeColumnName, customerIdColumnName = \"date_uct\", \"id\"\n",
    "\n",
    "if isWorkChat: \n",
    "    datetimeColumnName, customerIdColumnName = \"date\", \"name\"\n",
    "\n",
    "if isImessage:\n",
    "    datetimeColumnName, customerIdColumnName = \"date\", \"to_phone\"\n",
    "\n",
    "\n",
    "# setup date column\n",
    "df[\"datetime\"] = pd.to_datetime(df[datetimeColumnName], errors=\"coerce\")\n",
    "\n",
    "\n",
    "df[\"customer_id\"] = df[customerIdColumnName]\n",
    "\n",
    "# for workplace chat\n",
    "if isWorkChat:\n",
    "    df[\"is_from_me\"] = df.apply(lambda r:r.customer_id == \"Igor Dvorkin\",axis=1) # depends on \n",
    "df = df.set_index(df.datetime)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis -\n",
    "### Home Grown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# gotta be a more elegant way, but doing this for now\n",
    "\n",
    "ti = time_it(\"compute distribution\")\n",
    "distribs = [df[c].value_counts(normalize=True) for c in df.columns]\n",
    "ti.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isFlatDistribution(d):\n",
    "    return len(d) == 0 or d.iloc[0] < 0.01\n",
    "\n",
    "\n",
    "for d in sorted(\n",
    "    [d for d in distribs if not isFlatDistribution(d)], key=lambda d: d.iloc[0] * -1\n",
    "):\n",
    "    column_header = f\"\\n------ {d.name} ----- \"\n",
    "    print(column_header)\n",
    "    print(f\"{d.head(10)}\")\n",
    "\n",
    "print(\"++Flat distribution++\")\n",
    "for d in sorted([d for d in distribs if isFlatDistribution(d)], key=lambda d: d.name):\n",
    "    c = d.name\n",
    "    print(c)\n",
    "print(\"--Flat distribution--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis -\n",
    "### Like the grown ups do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks pretty crappy on a black background, maybe change color first\n",
    "# Also, I've had trouble with date indexes, might need to drop the index\n",
    "# df = df.reset_index(drop = True)\n",
    "profile_filename = \"output.html\"\n",
    "ti = time_it(f\"profiling dataframe to {profile_filename}\")\n",
    "#pr = df.reset_index(drop=True).profile_report()\n",
    "#pr.to_file(output_file=profile_filename)\n",
    "#pr\n",
    "ti.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_for(df, count_buckets, minimum_call_count=0):\n",
    "    cid = \"customer_id\"\n",
    "    usage_by_cid = df[cid].value_counts()\n",
    "    cid_to_exclude = usage_by_cid[\n",
    "        usage_by_cid.values <= minimum_call_count\n",
    "    ].index.values\n",
    "    df = df[~df.customer_id.isin(cid_to_exclude)]\n",
    "    usage_by_cid = df[cid].value_counts()\n",
    "    dfT = usage_by_cid.value_counts(normalize=True).toPercent().iloc[:count_buckets]\n",
    "    dfT.index.name = f\"% usages by customer\"\n",
    "    N = len(usage_by_cid.value_counts())\n",
    "    graphed = int(dfT.sum())\n",
    "    title = \"What % of time do customers call K times? \"\n",
    "    sub_title = f\"Universe customers calling >= {minimum_call_count} times, N={humanize.intcomma(N)}, Visible={graphed}%\"\n",
    "    ax = dfT.plot(kind=\"bar\", title=f\"{title}\\n{sub_title}\")\n",
    "    ax.set_ylabel(f\"% customers\")\n",
    "    plt.show()  # force the plot ot show\n",
    "\n",
    "\n",
    "# create a df w/only multi users for faster analysis\n",
    "def df_w_multi_customers(df):\n",
    "    usage_by_cid = df.customer_id.value_counts()\n",
    "    cid_to_exclude = usage_by_cid[usage_by_cid.values == 1].index.values\n",
    "    return df[~df.customer_id.isin(cid_to_exclude)]\n",
    "\n",
    "\n",
    "df_multi = df_w_multi_customers(df)\n",
    "\n",
    "ti = time_it(\"Plotting distributions\")\n",
    "plot_distribution_for(df, 3, 0)\n",
    "plot_distribution_for(df, 10, 3)\n",
    "plot_distribution_for(df, 10, 10)\n",
    "plot_distribution_for(df, 10, 500)\n",
    "ti.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# MAU/WAU/DAU analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "freq = \"M\"\n",
    "cid_by_date = df[\"2019\":].pivot_table(\n",
    "    values=\"datetime\",\n",
    "    index=[\"customer_id\"],\n",
    "    columns=pd.Grouper(freq=freq),\n",
    "    aggfunc=\"any\",\n",
    ")\n",
    "# cid_sorted_by_sum = cid_by_date.T.sum().sort_values(ascending=False).index\n",
    "# cid_by_date = cid_by_date.sort_values(\"customer_id\")\n",
    "# cid_by_date.T[cid_sorted_by_sum[4:5]].plot()\n",
    "# cid_by_date.T.count().sort_values()\n",
    "# df_hc.pivot_table(index=[\"customer_id\"], columns=pd.Grouper(freq=freq), aggfunc=\"count\")[ \"customer_id\" ].T.plot(title=f\"customer {irange} by freq={freq}\", figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cid_by_freq(df, freq):\n",
    "    return df[\"2019\":].pivot_table(\n",
    "        values=\"datetime\",\n",
    "        index=[\"customer_id\"],\n",
    "        columns=pd.Grouper(freq=freq),\n",
    "        aggfunc=\"any\",\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# cid_by_date.T.sum().sort_values(ascending=False)\n",
    "def print_freq(df, freq, cuteName, minUsage):\n",
    "    # minUsage should be inferred\n",
    "    c = cid_by_freq(df, freq).sum(axis=\"columns\")\n",
    "    print(f\"{cuteName}:{intcomma(len(c[c >= minUsage ]))}\")\n",
    "\n",
    "\n",
    "# NOTE: Could speed up significantly by removing\n",
    "# Single time users\n",
    "df_multi = df_w_multi_customers(df)\n",
    "print(f\"N:{intcomma(len(df.customer_id.value_counts()))}\")\n",
    "print(f\"N(>1):{intcomma(len(df_multi.customer_id.value_counts()))}\")\n",
    "print_freq(df_multi, \"M\", \"MAU_2\", 2)\n",
    "print_freq(df_multi, \"M\", \"MAU\", 5)\n",
    "print_freq(df_multi, \"W\", \"WAU\", 20)\n",
    "print_freq(df_multi, \"D\", \"DAU\", 140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df.resample(\"M\").count()[\"datetime\"].plot(title=\"Total usage over time\")\n",
    "plt.show()\n",
    "\n",
    "df_tr = df[\"2019\"]\n",
    "customer_by_count = df_tr.customer_id.value_counts()\n",
    "irange = range(0,5)\n",
    "df_hc = df_tr[df_tr.customer_id.isin(customer_by_count.index[irange].values)]\n",
    "\n",
    "top_customer_by_month =  df_hc.pivot_table(\n",
    "        values=\"datetime\",\n",
    "        index=[\"customer_id\"],\n",
    "        columns=pd.Grouper(freq=\"M\"),\n",
    "        aggfunc=\"count\",\n",
    "    fill_value=0\n",
    "        ).T\n",
    "\n",
    "for kind in \"line area bar\".split():\n",
    "    top_customer_by_month.plot(title=\"Top customers usage over time\", kind=kind)\n",
    "    plt.show()\n",
    "print(f\"customer_by_count\\n{customer_by_count.head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.customer_id==\"Sergio Wolman\"].datetime.dt.weekday.value_counts().orderby()\n",
    "#df = df[df.customer_id==\"Sergio Wolman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#  Resample to hours\n",
    "# Select weekdays between 6 and 19\n",
    "def filter_work_hours(df):\n",
    "    df = by_hour_count(df)\n",
    "    return df[(df.index.hour >5) & (df.index.hour <19) & (df.index.weekday <5)] \n",
    "\n",
    "def by_hour_count(df):\n",
    "    return df.resample('H').count().rename(columns={'customer_id':'count_messages'})\n",
    "\n",
    "def pivot_day_by_hour_no_melt(df,agg):\n",
    "    df = df.resample('H').count().rename(columns={'customer_id':'count_messages'})\n",
    "    pv = df.pivot_table(index=df.index.weekday, columns=df.index.hour,values=\"count_messages\",aggfunc=agg)\n",
    "    pv.index.name,  pv.columns.name = \"day\",\"hour\"\n",
    "    return pv\n",
    "\n",
    "\n",
    "def pivot_day_by_hour(df,agg):\n",
    "    pv = df.pivot_table(index=df.index.weekday, columns=df.index.hour,values=\"count_messages\",aggfunc=agg)\n",
    "    pv.index.name,  pv.columns.name = \"day\",\"hour\"\n",
    "    return pv\n",
    "\n",
    "\n",
    "def heat_map(df,agg, title,is_pivoted=False):\n",
    "    m =  df if is_pivoted else pivot_day_by_hour(df,agg)\n",
    "    m = m.reset_index().melt(id_vars=[\"day\"])\n",
    "    \n",
    "    # turn pivotted day/hour back to datetimes for altair plotting\n",
    "    start_of_week = date.today() - timedelta(date.today().weekday())\n",
    "    now = datetime.datetime(year=2000,month=1,day=1)\n",
    "    m.day = pd.to_datetime(m.day.apply(lambda x:start_of_week + timedelta(days=x)))\n",
    "    m.hour = pd.to_datetime(m.hour.apply(lambda x:now + timedelta(hours = now.hour + x)))\n",
    "    \n",
    "    hm =  alt.Chart(m).mark_rect().encode( \n",
    "        alt.Y('day(day):O'), \n",
    "        alt.X('hours(hour):O'),\n",
    "        color=alt.Color('value')\n",
    "    ).properties(title=title)\n",
    "    \n",
    "    text =  alt.Chart(m).mark_text().encode( \n",
    "        alt.Y('day(day):O'), \n",
    "        alt.X('hours(hour):O'),\n",
    "        alt.Text('value',format=\"d\"), # format as decimal\n",
    "        color=alt.value('white')\n",
    "    ).properties(title=title)\n",
    "    \n",
    "    return hm + text\n",
    "\n",
    "\n",
    "# hm =  draw_heat_map(t2,functools.partial(np.quantile,q=quantile),f\"{int(quantile*100)}th percentile\")\n",
    "sergio =  df[df.name=='Sergio Wolman']\n",
    "#heat_map(filter_work_hours(sergio), np.count, \"Sergio Time Count\")\n",
    "#display(hm)\n",
    "\n",
    "# from_ammon =  df[(df.customer_id=='+12063567091') & (df.is_from_me == False)]\n",
    "# to_ammon =  df[(df.customer_id=='+12063567091') & (df.is_from_me == True)]\n",
    "for q in [60,80, 95,99]:\n",
    "    agg = partial(np.quantile, q=q*0.01)\n",
    "    #f_ammon = pivot_day_by_hour_no_melt(from_ammon,agg)\n",
    "    #t_ammon = pivot_day_by_hour_no_melt(to_ammon,agg)\n",
    "    #delta_weight = t_ammon/(f_ammon+t_ammon).apply(lambda x:x*.01)\n",
    "    # display(heat_map(delta_weight,agg=None, is_pivoted=True, title=f\"% Igor Messages P{q}\"))\n",
    "    hm  =  heat_map(by_hour_count(sergio),agg,f\"Ammon P{q}\")\n",
    "    # display(hm)\n",
    "    \n",
    "for q in [50, 90, 95,99]:\n",
    "    agg = partial(np.quantile, q=q*0.01)\n",
    "    #f_ammon = pivot_day_by_hour_no_melt(from_ammon,agg)\n",
    "    #t_ammon = pivot_day_by_hour_no_melt(to_ammon,agg)\n",
    "    #delta_weight = t_ammon/(f_ammon+t_ammon).apply(lambda x:x*.01)\n",
    "    # display(heat_map(delta_weight,agg=None, is_pivoted=True, title=f\"% Igor Messages P{q}\"))\n",
    "    hm  =  heat_map(filter_work_hours(sergio),agg,f\"Sergio<->Igor chat messages @ P{q}\")\n",
    "    display(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
