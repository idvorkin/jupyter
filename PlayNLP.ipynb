{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore NLP against my journal entries\n",
    "\n",
    "This notebook allows me to play with NLP concepts using my personal journals.\n",
    "I've been writing personal journal entries ala 750 words a day for several years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import datasets, svm, metrics\n",
    "from pandas import DataFrame\n",
    "import matplotlib as mpl\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# get nltk and corpus\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get scapy and corpus\n",
    "import spacy\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from pandas_util import time_it\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "from datetime import timedelta\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# This function is in the first block so you don't\n",
    "# recreate it willy nilly, as it includes a cache.\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Remove domain words that don't help analysis.\n",
    "# Should be factored out\n",
    "domain_stop_words = set(\n",
    "    \"\"\"\n",
    "    yes yup Affirmations get that's Journal\n",
    "    Deliberate Disciplined Daily\n",
    "    Know Essential Provide Context\n",
    "    First Understand Appreciate\n",
    "    Hymn Monday\n",
    "    Grateful\n",
    "    ☐ ☑ K e tha Y X w\n",
    "    \"\"\".lower().split()\n",
    ")\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def get_nlp_model(model: str):\n",
    "    start_time = time.time()\n",
    "    print(f\"Loading Model {model}\")\n",
    "    nlp = spacy.load(model)  # python -m spacy download en_core_web_lg\n",
    "    spacy.prefer_gpu()  # This will be cool if/when it happens.\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Took: {int(duration)}\")\n",
    "    return nlp\n",
    "\n",
    "\n",
    "# Load corpus of my daily ramblings\n",
    "@dataclass(frozen=True)\n",
    "class Corpus:\n",
    "    path: str\n",
    "    all_content: str\n",
    "    initial_words: List[str]\n",
    "    words: List[str]\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.path.__hash__()\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def LoadCorpus(corpus_path: str) -> Corpus:\n",
    "\n",
    "    # Hym consider memoizing this asweel..\n",
    "    english_stop_words = set(stopwords.words(\"english\"))\n",
    "    all_stop_words = domain_stop_words | english_stop_words\n",
    "\n",
    "    corpus_path_expanded = os.path.expanduser(corpus_path)\n",
    "    corpus_files = glob.glob(corpus_path_expanded)\n",
    "\n",
    "    \"\"\"\n",
    "    ######################################################\n",
    "    # Performance side-bar.\n",
    "    ######################################################\n",
    "\n",
    "    A] Below code results in all strings Loaded into memory for temporary,  then merged into a second string.\n",
    "    aka Memory = O(2*file_conent) and CPU O(2*file_content)\n",
    "\n",
    "    B] An alternative is to do += on a string results in a new memory allocation and copy.\n",
    "    aka Memory = O(file_content) , CPU O(files*file_content)\n",
    "\n",
    "    However, this stuff needs to be measured, as it's also a funtion of GC. Not in the GC versions there is no change in CPU\n",
    "    Eg.\n",
    "\n",
    "    For A] if GC happens after every \"join\", then were down to O(file_content).\n",
    "    For B] if no GC, then it's still O(2*file_content)\n",
    "    \"\"\"\n",
    "\n",
    "    # Make single string from all the file contents.\n",
    "    list_file_content = [Path(file_name).read_text() for file_name in corpus_files]\n",
    "    all_file_content = \" \".join(list_file_content)\n",
    "\n",
    "    # NOTE I can Upper case Magic to make it a proper noun and see how it ranks!\n",
    "\n",
    "    properNouns = \"zach ammon Tori amelia josh Ray javier Neha Amazon John\".split()\n",
    "\n",
    "    def capitalizeProperNouns(s: str):\n",
    "        for noun in properNouns:\n",
    "            noun = noun.lower()\n",
    "            properNoun = noun[0].upper() + noun[1:]\n",
    "            s = s.replace(\" \" + noun, \" \" + properNoun)\n",
    "        return s\n",
    "\n",
    "    # Grr- Typo replacer needs to be lexically smart - sigh\n",
    "    typos = [\n",
    "        (\"waht\", \"what\"),\n",
    "        ('I\"ll', \"I'll\"),\n",
    "        (\"that\", \"that\"),\n",
    "        (\"taht\", \"that\"),\n",
    "        (\"ti \", \"it \"),\n",
    "        (\"that'sa\", \"that's a\"),\n",
    "        (\"Undersatnd\", \"Understand\"),\n",
    "        (\" Ill \", \" I'll \"),\n",
    "        (\"Noitce\", \"Notice\"),\n",
    "        (\"Whcih\", \"Which\"),\n",
    "        (\" K \", \" OK \"),\n",
    "        (\"sTories\", \"stories\"),\n",
    "        (\"htat\", \"that\"),\n",
    "        (\"Getitng\", \"Getting\"),\n",
    "        (\"Essenital\", \"Essential\"),\n",
    "        (\"whcih\", \"which\"),\n",
    "        (\" nad \", \" and \"),\n",
    "        (\" adn \", \" and \"),\n",
    "    ]\n",
    "\n",
    "    def fixTypos(s: str):\n",
    "        for typo in typos:\n",
    "            s = s.replace(\" \" + typo[0], \" \" + typo[1])\n",
    "        return s\n",
    "\n",
    "    all_file_content = fixTypos(all_file_content)\n",
    "    all_file_content = capitalizeProperNouns(all_file_content)\n",
    "\n",
    "    # Clean out some punctuation (although does that mess up stemming later??)\n",
    "    initial_words = all_file_content.replace(\",\", \" \").replace(\".\", \" \").split()\n",
    "\n",
    "    words = [word for word in initial_words if word.lower() not in all_stop_words]\n",
    "    return Corpus(\n",
    "        path=corpus_path,\n",
    "        all_content=all_file_content,\n",
    "        initial_words=initial_words,\n",
    "        words=words,\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def DocForCorpus(nlp, corpus: Corpus):\n",
    "    print(\n",
    "        f\"initial words {len(corpus.initial_words)} remaining words {len(corpus.words)}\"\n",
    "    )\n",
    "    ti = time_it(f\"Building corpus from {corpus.path} of len:{len(corpus.all_content)} \")\n",
    "    # We use all_file_content not initial_words because we want to keep punctuation.\n",
    "    doc_all = nlp(corpus.all_content)\n",
    "\n",
    "    # Remove domain specific stop words.\n",
    "    doc = [token for token in doc_all if token.text.lower() not in domain_stop_words]\n",
    "    ti.stop()\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build corpus from my journal in igor2/750words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# make function path for y/m\n",
    "\n",
    "\n",
    "# Build up corpus reader.\n",
    "# Current design, hard code knowledge of where everything is stored in forwards direction.\n",
    "# Hard code insufficient data for analysis.\n",
    "# Better model, read all files in paths and then do lookup.\n",
    "\n",
    "\n",
    "\n",
    "# Hymn better model:\n",
    "# A - lookup all files.\n",
    "# B - Generate paths based on actual locations.\n",
    "\n",
    "def glob750_latest(year, month):\n",
    "    assert month in range(1, 13)\n",
    "    base750 = \"~/gits/igor2/750words/\"\n",
    "    return f\"{base750}/{year}-{month:02}-*.md\"\n",
    "\n",
    "def glob750_new_archive(year, month):\n",
    "    assert month in range(1, 13)\n",
    "    base750 = \"~/gits/igor2/750words_new_archive/\"\n",
    "    return f\"{base750}/{year}-{month:02}-*.md\"\n",
    "\n",
    "\n",
    "def glob750_old_archive(year, month):\n",
    "    assert month in range(1, 13)\n",
    "    base750archive = \"~/gits/igor2/750words_archive/\"\n",
    "    return f\"{base750archive}/750 Words-export-{year}-{month:02}-01.txt\"\n",
    "\n",
    "\n",
    "def corpus_paths_months_for_year(year):\n",
    "    return [glob750_old_archive(year, month) for month in range(1, 13)]\n",
    "\n",
    "\n",
    "# Corpus in \"old archieve\"  from 2012-2017.\n",
    "corpus_path_months = {\n",
    "    year: corpus_paths_months_for_year(year) for year in range(2012, 2018)\n",
    "}\n",
    "\n",
    "# 2018 Changes from old archive to new_archieve.\n",
    "# 2018 Jan/Feb/October don't have enough data for analysis\n",
    "corpus_path_months[2018] = [glob750_old_archive(2018, month) for month in range(3, 8)] + [\n",
    "    glob750_new_archive(2018, month) for month in (9, 11, 12)\n",
    "]\n",
    "\n",
    "corpus_path_months[2019] = [glob750_new_archive(2019, month) for month in range(1, 13)]\n",
    "corpus_path_months[2020] = [glob750_new_archive(2020, month) for month in range(1, 11)] # TBD compute month progratically\n",
    "\n",
    "corpus_path_months_trailing = [\n",
    "    glob750_new_archive(2018, month) for month in (9, 11, 12)\n",
    "] + corpus_path_months[2019] + corpus_path_months[2020]\n",
    "\n",
    "corpus_path_months_trailing\n",
    "\n",
    "# TODO: Add a pass to remove things with insufficient words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the plot wider\n",
    "height_in_inches = 8\n",
    "matplotlib.rc(\"figure\", figsize=(2 * height_in_inches, height_in_inches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load simple corpus for my journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = LoadCorpus(corpus_path_months[2020][-1])\n",
    "print(f\"initial words {len(corpus.initial_words)} remaining words {len(corpus.words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use nltk frequency distribution plot, but better off building our own.\n",
    "# fd = nltk.FreqDist(words)\n",
    "# fd.plot(50, percents=True)\n",
    "# Can also use scikit learn CountVectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as NLTK FreqDist, except normalized, includes cumsum, and colors\n",
    "def GraphWordDistribution(words, title=\"\", skip=0, length=50, includeCDF=True) -> None:\n",
    "    def GetPDFCDF(words):\n",
    "        def ToPercent(x: float) -> float:\n",
    "            return x * 100\n",
    "\n",
    "        # NOTE: No point creating a full data frame when only using a single column.\n",
    "        pdf = pd.Series(words).value_counts(normalize=True).apply(ToPercent)\n",
    "        cdf = pdf.cumsum()\n",
    "        return (pdf, cdf)\n",
    "\n",
    "    def PlotOnAxis(series, ax, label: str, color: str):\n",
    "        # RANT: Why is MPL so confusing? The OO interface vs the stateful interface, GRAH!!\n",
    "        # The random non-obvious calls.\n",
    "        # GRAH!!!\n",
    "\n",
    "        ax.legend(label.split())\n",
    "        ax.plot(series, color=color)\n",
    "\n",
    "        # RANT: Why no YAxis.set_labal_params()? E.g.\n",
    "        #                 ax.yaxis.set_label_params(label, color=color)\n",
    "        ax.set_ylabel(label, color=color)\n",
    "        ax.yaxis.set_tick_params(labelcolor=color)\n",
    "\n",
    "        # technically all the X axis paramaters are duplicated since we \"twinned the X paramater\"\n",
    "        ax.xticks = range(len(series))\n",
    "\n",
    "        # RANT: rot can be set on plt.plot(), but not on axes.plot()\n",
    "        ax.xaxis.set_tick_params(rotation=90)\n",
    "\n",
    "    # NOTE: can make graph prettier with styles E.g.\n",
    "    # with plt.style.context(\"ggplot\"):\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # make pdf first axes, and cdf second axes.\n",
    "    ax_pdf, ax_cdf = (ax, ax.twinx())\n",
    "    color_pdf, color_cdf = (\"green\", \"blue\")\n",
    "    pdf, cdf = GetPDFCDF(words)\n",
    "\n",
    "    PlotOnAxis(pdf[skip : skip + length], ax_pdf, label=\"PDF*100\", color=color_pdf)\n",
    "    PlotOnAxis(cdf[skip : skip + length], ax_cdf, label=\"CDF*100\", color=color_cdf)\n",
    "\n",
    "\n",
    "GraphWordDistribution(corpus.words, title=\"Normalized Word Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 10\n",
    "GraphWordDistribution(\n",
    "    corpus.words, skip=skip, length=75, title=f\"Distribution without top {skip} words\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud is non-deterministic, which is bizarre.\n",
    "# from wordcloud import WordCloud\n",
    "# wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", stopwords=None).generate(\"\".join(words))\n",
    "# plt.imshow(wordcloud,  interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with POS tagging and lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = get_nlp_model(\"en_core_web_lg\")\n",
    "nlp.max_length = 100 * 1000 * 1000\n",
    "\n",
    "\n",
    "def GetInterestingWords(pos: str, doc, corpus: Corpus):\n",
    "    interesting_pos = pos\n",
    "    interesting_pos_set = set(interesting_pos.split())\n",
    "    interesting = [token for token in doc if token.pos_ in interesting_pos_set]\n",
    "    interesting_words = [token.lemma_ for token in interesting]\n",
    "    return interesting_words\n",
    "\n",
    "\n",
    "def GraphPoSForDoc(pos: str, doc, corpus):\n",
    "    GraphWordDistribution(\n",
    "        GetInterestingWords(pos, doc, corpus=corpus),\n",
    "        title=f\"Distribution of {pos} on {corpus.path}\",\n",
    "        skip=0,\n",
    "        length=20,\n",
    "    )\n",
    "\n",
    "\n",
    "def GraphScratchForCorpus(corpus_path: str, pos: str = \"NOUN VERB ADJ ADV\"):\n",
    "    corpus = LoadCorpus(corpus_path)\n",
    "    doc = DocForCorpus(nlp, corpus)\n",
    "    GraphPoSForDoc(pos, doc, corpus)\n",
    "\n",
    "\n",
    "def GetInterestingForCorpusPath(corpus_path: str, pos: str = \"NOUN VERB ADJ ADV\"):\n",
    "    corpus = LoadCorpus(corpus_path)\n",
    "    doc = DocForCorpus(nlp, corpus)\n",
    "    return GetInterestingWords(pos, doc, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# corpus_paths = corpus_paths_years\n",
    "corpus_paths = corpus_path_months[2020]\n",
    "print(corpus_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus_paths:\n",
    "    GraphScratchForCorpus(c, pos=\"PROPN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging when stuff goes goofy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "max_to_analyze = 15\n",
    "interesting = [token for token in doc if token.tag_ == \"NNS\"]\n",
    "for token in interesting[:max_to_analyze]:\n",
    "    # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)\n",
    "\n",
    "# Parts of speech: https://spacy.io/usage/linguistic-features\n",
    "GraphWordDistribution([token.pos_ for token in doc], title=f\"POS Distribution on {corpus_path}\")\n",
    "# interesting = [ token for token in doc if token.pos_ != \"PUNCT\" and token.pos_ != \"SYM\" and len(token.text) > 3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the \"Thought Distribution\" over time.\n",
    "* A] Sentiment over time. Graph valence as line graph time series\n",
    "    (TBD: Use cloud service to analyze each file)\n",
    "\n",
    "* B] Graph a bar chart of Proper noun trending over time, have it update per corpus file.\n",
    " * Build a data frame of word frequency \"Proper Noun\"x\"Corpus\"\n",
    " * Graph update every second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakePDF(words, name):\n",
    "    def ToPercent(x: float) -> float:\n",
    "        return x * 100\n",
    "\n",
    "    return pd.Series(words, name=name).value_counts(normalize=True).apply(ToPercent)\n",
    "\n",
    "\n",
    "def PathToFriendlyTitle(path: str):\n",
    "    path = path.split(\"/\")[-1]\n",
    "    if \"export-\" in path:\n",
    "        return path.split(\"export-\")[-1]\n",
    "    else:\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_paths = corpus_path_months[2018]+corpus_path_months[2019]\n",
    "# corpus_paths = corpus_path_months[2018] + corpus_path_months[2019]\n",
    "corpus_paths = corpus_path_months_trailing[-12:]\n",
    "top_words_to_skip,   count_words   = 0, 10\n",
    "print(corpus_paths)\n",
    "pdfs = [\n",
    "    MakePDF(GetInterestingForCorpusPath(p, \"PROPN\"), PathToFriendlyTitle(p))\n",
    "    for p in corpus_paths\n",
    "]\n",
    "\n",
    "# TODO: Why can't we use the join - gives an error.\n",
    "# wordByTimespan = pd.DataFrame().join(pdfs, how=\"outer\", sort=False)\n",
    "wordByTimespan = pd.DataFrame()\n",
    "for pdf in pdfs:\n",
    "    wordByTimespan = wordByTimespan.join(pdf, how=\"outer\")\n",
    "\n",
    "# Sort by sum(word frequency) over all corpus\n",
    "# I  suspect it'd be interesting to sort by TF*IDF because it'll make words that are present\n",
    "# only in a few months get a boost.\n",
    "wordByTimespan[\"word_frequency\"] = wordByTimespan.sum(skipna=True, axis=\"columns\")\n",
    "wordByTimespan = wordByTimespan.sort_values(\"word_frequency\", ascending=False)\n",
    "\n",
    "\n",
    "# Remove total column\n",
    "wordByTimespan = wordByTimespan.iloc[:, :-1]\n",
    "\n",
    "print (f\"skipping:{top_words_to_skip}, count:{count_words} \")\n",
    "\n",
    "# wordByTimespan.iloc[:50, :].plot( kind=\"bar\", subplots=False, legend=False, figsize=(15, 14), sharey=True )\n",
    "wordByTimespan.iloc[top_words_to_skip:top_words_to_skip + count_words, :].T.plot(\n",
    "    kind=\"bar\", subplots=True, legend=False, figsize=(15, 9), sharey=True\n",
    ")\n",
    "# wordByTimespan.iloc[:13, :].T.plot( kind=\"bar\", subplots=False, legend=True, figsize=(15, 14), sharey=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_to_skip, count_words = 0,50\n",
    "top_word_by_year = wordByTimespan.iloc[top_words_to_skip:top_words_to_skip + count_words, :][::-1]\n",
    "# top_word_by_year = wordByTimespan.iloc[:15,:][::-1] # the -1 on the end reverse the count\n",
    "\n",
    "anim_fig_size=(16,20)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax = top_word_by_year.iloc[:,0].plot(\n",
    "    title=f\"Title Over Written\", figsize=anim_fig_size,  kind='barh'\n",
    ")\n",
    "\n",
    "animation.patches = ax.patches\n",
    "loop_colors = itertools.cycle('bgrcmk')\n",
    "animation.colors = list(itertools.islice(loop_colors,len(animation.patches)))\n",
    "\n",
    "\n",
    "def animate(i, ):\n",
    "    # OMG: That was impossible to find!!!\n",
    "    # Turns out every time you call plot, more patches (bars) are added to graph.  You need to remove them, which is very non-obvious.\n",
    "    # https://stackoverflow.com/questions/49791848/matplotlib-remove-all-patches-from-figure\n",
    "    [p.remove() for p in reversed(animation.patches)]\n",
    "    top_word_by_year.iloc[:,i].plot(title=f\"Distribution {top_word_by_year.columns[i]}\", kind='barh', color=animation.colors , xlim=(0,10))\n",
    "    return (animation.patches,)\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, animate, frames=len(top_word_by_year.columns), interval=timedelta(seconds=1).seconds * 1000, blit=False\n",
    ")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmo = \"\"\"\n",
    "corpus_path = \"~/gits/igor2/750words/2019-06-*md\"\n",
    "corpus = LoadCorpus(corpus_path)\n",
    "doc = DocForCorpus(nlp, corpus)\n",
    "for t in doc[400:600]:\n",
    "print(f\"{t} {t.lemma_} {t.pos_}\")\n",
    "\"\"\"\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(nlp(\"Igor wonders if Ray is working too much\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
